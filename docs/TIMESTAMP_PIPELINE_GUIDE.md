# æ—¶é—´æˆ³åå¤„ç† Pipeline ä½¿ç”¨æŒ‡å—

## ğŸ¯ æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ª**é›¶æ ‡æ³¨ã€é›¶è®­ç»ƒ**çš„æ—¶é—´æˆ³è‡ªåŠ¨å¯¹é½æ–¹æ¡ˆï¼Œé€šè¿‡åå¤„ç†ä¸ºæ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ–‡æœ¬è‡ªåŠ¨æ·»åŠ å¸§çº§æ—¶é—´æˆ³ã€‚

**æ ¸å¿ƒæµç¨‹**ï¼š
```
è§†é¢‘ â†’ é‡‡å¸§ â†’ æ¨¡å‹æ¨ç† â†’ äº‹ä»¶æå– â†’ CLIPåŒ¹é… â†’ æ—¶é—´æˆ³æ’å…¥
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# æ ¸å¿ƒä¾èµ–
pip install open-clip-torch  # CLIP æ¨¡å‹
pip install decord           # å¿«é€Ÿè§†é¢‘è¯»å–ï¼ˆæ¨èï¼‰
# æˆ–è€…ä½¿ç”¨ opencv-python ä½œä¸ºå¤‡é€‰

# å¦‚æœä½¿ç”¨åŸç‰ˆ CLIP
pip install git+https://github.com/openai/CLIP.git
```

### 2. è¿è¡Œæµ‹è¯•

```bash
cd /data2/youle/HumanOmniV2/spatio-temporal-reasoner/scripts
python test_timestamp_pipeline.py
```

**é¢„æœŸè¾“å‡º**ï¼š
- è§†é¢‘ä¿¡æ¯å’Œé‡‡æ ·å¸§æ•°
- åŸå§‹ `<think>` å†…å®¹
- æå–çš„äº‹ä»¶åˆ—è¡¨
- æ¯ä¸ªäº‹ä»¶åŒ¹é…çš„å¸§å·å’Œæ—¶é—´æˆ³
- å¸¦æ—¶é—´æˆ³çš„æœ€ç»ˆ `<think>` å†…å®¹

### 3. æŸ¥çœ‹ç»“æœ

ç»“æœä¼šä¿å­˜åœ¨ `../logs/timestamp_pipeline_YYYYMMDD_HHMMSS.json`

## ğŸ“š æ¨¡å—è¯´æ˜

### tools/video_utils.py

**åŠŸèƒ½**ï¼šç»Ÿä¸€çš„è§†é¢‘é‡‡å¸§

```python
from tools.video_utils import sample_frames

frames_pil, frame_ids, timestamps, fps = sample_frames(
    video_path="video.mp4",
    max_frames=32,      # ä¸æ¨¡å‹æ¨ç†ä¸€è‡´
    strategy="uniform"  # å‡åŒ€é‡‡æ ·
)
```

**å…³é”®**ï¼š`max_frames` å¿…é¡»ä¸æ¨¡å‹æ¨ç†æ—¶ä¸€è‡´ï¼

---

### tools/clip_matcher.py

**åŠŸèƒ½**ï¼šCLIP å›¾æ–‡åŒ¹é… + å•è°ƒçº¦æŸ

```python
from tools.clip_matcher import CLIPMatcher, match_with_monotonic_constraint

# åˆå§‹åŒ–
matcher = CLIPMatcher(model_name="ViT-B-32", device="cuda")

# æ–¹æ³•1: ç‹¬ç«‹åŒ¹é…
matches = matcher.match_events_to_frames(
    events=["woman picks up rose", "man smiles"],
    frames_pil=frames_pil
)
# ç»“æœ: {"woman picks up rose": 3, "man smiles": 8}

# æ–¹æ³•2: å•è°ƒçº¦æŸï¼ˆæ¨èï¼‰
similarity_matrix = matcher.get_similarity_matrix(events, frames_pil)
best_frames = match_with_monotonic_constraint(
    similarity_matrix,
    lambda_smooth=0.3  # å¹³æ»‘ç³»æ•°
)
# ç»“æœ: [3, 8]ï¼ˆä¿è¯éé€’å‡ï¼‰
```

**å‚æ•°è°ƒä¼˜**ï¼š
- `lambda_smooth=0.1~0.5`: è¶Šå¤§è¶Šå¹³æ»‘ï¼ˆé¿å…è·³è·ƒï¼‰
- CLIP æ¨¡å‹ï¼š`ViT-B-32`ï¼ˆå¿«ï¼‰vs `ViT-L-14`ï¼ˆå‡†ï¼‰

---

### scripts/extract_events.py

**åŠŸèƒ½**ï¼šä» `<think>` æå–å…³é”®äº‹ä»¶

```python
from extract_events import extract_events

# æ–¹æ³•1: LLM æå–ï¼ˆæ¨èï¼‰
events = extract_events(
    think_text=think,
    method="llm",
    model=model,
    processor=processor,
    max_events=10
)

# æ–¹æ³•2: è§„åˆ™æå–ï¼ˆFallbackï¼‰
events = extract_events(
    think_text=think,
    method="rule",
    max_events=10
)

# ç»“æœ: [Event(anchor="...", query="..."), ...]
```

**Event ç»“æ„**ï¼š
- `anchor`: ç”¨äºåœ¨åŸæ–‡ä¸­å®šä½ï¼ˆä¿æŒåŸå¥ï¼‰
- `query`: ç”¨äº CLIP åŒ¹é…ï¼ˆçŸ­ã€è§†è§‰åŒ–ï¼‰

---

### scripts/insert_timestamps.py

**åŠŸèƒ½**ï¼šå°†æ—¶é—´æˆ³æ’å…¥åŸæ–‡

```python
from insert_timestamps import insert_timestamps

result = insert_timestamps(
    think_text=original_think,
    events=events,
    frame_matches={"woman picks up rose": 3, ...},
    timestamps=[0.0, 1.0, 2.0, ...],
    format_style="frame_and_time"  # [Frame 3: 3.00s]
)
```

**æ ¼å¼é€‰é¡¹**ï¼š
- `"frame_and_time"`: `[Frame 3: 3.00s]`
- `"frame_only"`: `[Frame 3]`
- `"time_only"`: `[3.00s]`

---

## ğŸ”§ å‚æ•°é…ç½®

### æ¨èé…ç½®ï¼ˆtest_timestamp_pipeline.pyï¼‰

```python
MAX_FRAMES = 32              # è§†é¢‘é‡‡æ ·å¸§æ•°ï¼ˆä¸æ¨ç†ä¸€è‡´ï¼‰
CLIP_MODEL = "ViT-B-32"      # CLIP æ¨¡å‹
USE_MONOTONIC_CONSTRAINT = True  # ä½¿ç”¨å•è°ƒçº¦æŸ
LAMBDA_SMOOTH = 0.3          # å¹³æ»‘ç³»æ•°
```

### è°ƒä¼˜å»ºè®®

| é—®é¢˜ | è°ƒæ•´ |
|------|------|
| æ—¶é—´æˆ³è·³è·ƒå¤ªå¤§ | å¢å¤§ `LAMBDA_SMOOTH` (0.3 â†’ 0.5) |
| åŒ¹é…ä¸å‡†ç¡® | æ¢ç”¨ `ViT-L-14` æˆ–å¢åŠ  `max_frames` |
| äº‹ä»¶æå–ä¸å…¨ | å¢å¤§ `max_events` æˆ–æ”¹è¿›äº‹ä»¶æå– Prompt |
| æ’å…¥å¤±è´¥ç‡é«˜ | æ£€æŸ¥ anchor æ˜¯å¦åœ¨åŸæ–‡ä¸­ï¼ˆå¯èƒ½éœ€è¦æ”¹è¿›äº‹ä»¶æå–ï¼‰|

---

## ğŸ“Š æ•ˆæœè¯„ä¼°

### è¯„ä¼°æŒ‡æ ‡

1. **æ—¶é—´æˆ³æ’å…¥ç‡**: æˆåŠŸæ’å…¥çš„äº‹ä»¶ / æå–çš„äº‹ä»¶
   - ç›®æ ‡: â‰¥ 70%
   
2. **æ—¶åºä¸€è‡´æ€§**: æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´å€’æµ
   - ä½¿ç”¨å•è°ƒçº¦æŸåº” 100% æ»¡è¶³

3. **åŒ¹é…å‡†ç¡®æ€§**: äººå·¥æ£€æŸ¥å¸§å·æ˜¯å¦åˆç†
   - ç²—ç²’åº¦ï¼ˆÂ±2å¸§ï¼‰: 80-90%
   - ç²¾ç¡®ï¼ˆå‡†ç¡®å¸§ï¼‰: 60-70%

### æµ‹è¯•å‘½ä»¤

```bash
# å•ä¸ªæ ·æœ¬æµ‹è¯•
python test_timestamp_pipeline.py

# æ‰¹é‡æµ‹è¯•ï¼ˆéœ€è‡ªå·±å®ç°ï¼‰
python batch_test_timestamps.py --num_samples 20
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: äº‹ä»¶æå–å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ `<think>` å†…å®¹ï¼š
- å¦‚æœ `<think>` å¤ªçŸ­æˆ–ä¸åŒ…å«äº‹ä»¶æè¿° â†’ æ”¹è¿› System Prompt
- å¦‚æœ LLM æå–å¤±è´¥ â†’ ä¼šè‡ªåŠ¨ fallback åˆ°è§„åˆ™æ–¹æ³•
- å¦‚æœè§„åˆ™æ–¹æ³•ä¹Ÿä¸è¡Œ â†’ å¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´å…³é”®è¯åˆ—è¡¨

### Q2: CLIP åŒ¹é…ä¸å‡†ç¡®ï¼Ÿ

**A**: å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š
1. **äº‹ä»¶æè¿°å¤ªæŠ½è±¡** â†’ æ”¹è¿› query çŸ­è¯­ï¼ˆæ›´å…·ä½“ã€æ›´è§†è§‰åŒ–ï¼‰
2. **è§†é¢‘è´¨é‡å·®** â†’ å¢åŠ é‡‡æ ·å¸§æ•° `max_frames`
3. **CLIP æ¨¡å‹ä¸å¤Ÿå¼º** â†’ æ¢ç”¨ `ViT-L-14`

### Q3: æ—¶é—´æˆ³æ’å…¥å¤±è´¥ï¼Ÿ

**A**: æ£€æŸ¥æ—¥å¿—ä¸­çš„ `verification` ç»“æœï¼š
- å¦‚æœ `insertion_rate < 0.5` â†’ anchor ä¸åŸæ–‡ä¸åŒ¹é…
  - è§£å†³ï¼šæ”¹è¿›äº‹ä»¶æå–çš„ anchor è´¨é‡
  - æˆ–è€…ä½¿ç”¨æ›´å®½æ¾çš„æ¨¡ç³ŠåŒ¹é…

### Q4: æ²¡æœ‰ decord æ€ä¹ˆåŠï¼Ÿ

**A**: è‡ªåŠ¨ fallback åˆ° cv2ï¼ˆopencv-pythonï¼‰
```bash
pip install opencv-python
```

### Q5: å†…å­˜ä¸è¶³ï¼Ÿ

**A**: 
1. å‡å°‘ `max_frames` (32 â†’ 16)
2. ä½¿ç”¨æ›´å°çš„ CLIP æ¨¡å‹ (`ViT-B-32` â†’ `RN50`)
3. æ‰¹é‡å¤„ç†æ—¶é€ä¸ªå¤„ç†æ ·æœ¬

---

## ğŸ¨ è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰äº‹ä»¶æå– Prompt

ç¼–è¾‘ `scripts/extract_events.py` ä¸­çš„ `extract_events_with_llm` å‡½æ•°ï¼š

```python
prompt = f"""Your custom prompt here...

Extract visual events from:
{think_text}

Output JSON:
{{"events": [...]}}
"""
```

### é›†æˆåˆ°ç”Ÿäº§æµç¨‹

```python
# åœ¨ä½ çš„æ¨ç†è„šæœ¬ä¸­
from tools.video_utils import sample_frames
from tools.clip_matcher import CLIPMatcher
from extract_events import extract_events
from insert_timestamps import insert_timestamps

# 1. æ¨ç†
think = run_inference(...)

# 2. é‡‡å¸§
frames, _, timestamps, _ = sample_frames(video_path, max_frames=32)

# 3. äº‹ä»¶æå–
events = extract_events(think, method="llm", model=model, processor=processor)

# 4. CLIP åŒ¹é…
matcher = CLIPMatcher()
matches = matcher.match_events_to_frames([e.query for e in events], frames)

# 5. æ’å…¥æ—¶é—´æˆ³
result = insert_timestamps(think, events, matches, timestamps)
```

---

## ğŸ“ˆ æ€§èƒ½å‚è€ƒ

**æµ‹è¯•ç¯å¢ƒ**: A100 40GB

| é˜¶æ®µ | è€—æ—¶ | å¤‡æ³¨ |
|------|------|------|
| è§†é¢‘é‡‡å¸§ (32 å¸§) | ~0.2s | decord |
| æ¨¡å‹æ¨ç† | ~5-10s | HumanOmniV2 |
| äº‹ä»¶æå– (LLM) | ~2-3s | ç”ŸæˆçŸ­æ–‡æœ¬ |
| CLIP åŒ¹é… (10 äº‹ä»¶) | ~0.1s | ViT-B-32 |
| æ—¶é—´æˆ³æ’å…¥ | <0.01s | çº¯å­—ç¬¦ä¸²æ“ä½œ |
| **æ€»è®¡** | **~8-13s** | å•ä¸ªæ ·æœ¬ |

---

## ğŸ“ TODO

- [ ] æ‰¹é‡æµ‹è¯•è„šæœ¬
- [ ] è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡
- [ ] å¯è§†åŒ–å·¥å…·ï¼ˆæ˜¾ç¤ºå¸§+æ—¶é—´æˆ³ï¼‰
- [ ] SAM3 mask é›†æˆ
- [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆæ‰¹å¤„ç†ã€ç¼“å­˜ï¼‰

---

## ğŸ¤ è´¡çŒ®

å¦‚æœä½ å‘ç°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issue æˆ– PRï¼

ä¸»è¦æ”¹è¿›æ–¹å‘ï¼š
1. æ›´å¥½çš„äº‹ä»¶æå–ç®—æ³•
2. æ›´å‡†ç¡®çš„ CLIP åŒ¹é…
3. æ›´é²æ£’çš„æ—¶é—´æˆ³æ’å…¥
