#!/usr/bin/env python3
"""
æŒ‡ä»¤éµå¾ªèƒ½åŠ›æµ‹è¯•è„šæœ¬
å¯¹åŒä¸€ä¸ªè§†é¢‘ä½¿ç”¨ä¸åŒçš„promptï¼Œæµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸçš„éµå¾ªæŒ‡ä»¤
"""

import os
import sys
import json
import yaml
import random
import torch
import re
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcè·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src" / "src"))

from transformers import AutoProcessor, Qwen2_5OmniThinkerForConditionalGeneration
from qwen_omni_utils import process_mm_info

# è®¾ç½®éšæœºç§å­
random.seed(42)
torch.manual_seed(42)


# ==================== æµ‹è¯•ç”¨çš„ä¸åŒæŒ‡ä»¤ï¼ˆ3ç§ä»£è¡¨æ€§æµ‹è¯•ï¼‰====================
TEST_PROMPTS = {
    "count_3_points": {
        "name": "æŒ‡å®šæ•°é‡ï¼š3ä¸ªè¦ç‚¹",
        "instruction": "Please summarize this video in exactly 3 key points.",
        "expected": "åº”è¯¥è¾“å‡º3ä¸ªè¦ç‚¹"
    },
    
    "focus_people": {
        "name": "æŒ‡å®šå…³æ³¨ç‚¹ï¼šäººç‰©",
        "instruction": "Describe the people in this video, focusing on their actions, expressions, and interactions.",
        "expected": "åº”è¯¥è¯¦ç»†æè¿°äººç‰©"
    },
    
    "one_sentence": {
        "name": "æŒ‡å®šé•¿åº¦ï¼šä¸€å¥è¯",
        "instruction": "Describe this video in one single sentence.",
        "expected": "åº”è¯¥åªæœ‰ä¸€å¥è¯"
    },
}


def load_dataset(yaml_path):
    """åŠ è½½è®­ç»ƒæ•°æ®é›†"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {yaml_path}")
    
    with open(yaml_path, 'r') as f:
        yaml_data = yaml.safe_load(f)
    
    datasets = yaml_data.get('datasets', [])
    all_samples = []
    
    for dataset_config in datasets:
        json_path = dataset_config.get('json_path')
        data_root = dataset_config.get('data_root')
        
        print(f"  â”œâ”€ åŠ è½½: {json_path}")
        
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # æ·»åŠ data_rootè·¯å¾„
        if data_root:
            for sample in data:
                if 'path' in sample:
                    sample['path'] = os.path.join(data_root, sample['path'])
        
        all_samples.extend(data)
        print(f"  â””â”€ åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")
    
    print(f"\nâœ… æ€»å…±åŠ è½½ {len(all_samples)} ä¸ªæ ·æœ¬\n")
    return all_samples


def create_messages_with_custom_prompt(sample, custom_instruction, system_prompt):
    """åˆ›å»ºè‡ªå®šä¹‰æŒ‡ä»¤çš„å¯¹è¯æ¶ˆæ¯"""
    
    # æ„é€ messages
    messages = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": system_prompt
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": sample.get('data_type', 'video'),
                    sample.get('data_type', 'video'): sample['path'],
                    "max_frames": 32,
                    "max_pixels": 602112
                },
                {
                    "type": "text",
                    "text": custom_instruction
                }
            ]
        }
    ]
    
    return messages


def extract_tags(text):
    """æå–<context>, <think>, <answer>æ ‡ç­¾å†…å®¹"""
    context_match = re.search(r'<context>(.*?)</context>', text, re.DOTALL)
    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)
    answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)
    
    return {
        'context': context_match.group(1).strip() if context_match else None,
        'think': think_match.group(1).strip() if think_match else None,
        'answer': answer_match.group(1).strip() if answer_match else None
    }


def count_sentences(text):
    """ç»Ÿè®¡å¥å­æ•°é‡"""
    if not text:
        return 0
    sentences = re.split(r'[.!?]+', text)
    return len([s for s in sentences if s.strip()])


def count_list_items(text):
    """ç»Ÿè®¡åˆ—è¡¨é¡¹æ•°é‡"""
    if not text:
        return 0
    # åŒ¹é…ç¼–å·åˆ—è¡¨ (1. 2. 3.) æˆ– é¡¹ç›®ç¬¦å· (- * â€¢)
    patterns = [
        r'^\d+\.',  # 1. 2. 3.
        r'^[-*â€¢]',  # - * â€¢
    ]
    
    lines = text.split('\n')
    count = 0
    for line in lines:
        line = line.strip()
        for pattern in patterns:
            if re.match(pattern, line):
                count += 1
                break
    return count


def analyze_output(generated_text, prompt_config):
    """åˆ†æè¾“å‡ºæ˜¯å¦ç¬¦åˆæŒ‡ä»¤"""
    extracted = extract_tags(generated_text)
    analysis = {
        "has_context": extracted['context'] is not None,
        "has_think": extracted['think'] is not None,
        "has_answer": extracted['answer'] is not None,
    }
    
    answer_text = extracted.get('answer', '')
    
    # æ ¹æ®ä¸åŒçš„promptç±»å‹è¿›è¡Œåˆ†æ
    prompt_key = prompt_config.get('key', '')
    
    if 'count_3' in prompt_key:
        list_count = count_list_items(answer_text)
        analysis['list_items'] = list_count
        analysis['follows_instruction'] = (list_count == 3)
        analysis['note'] = f"è¦æ±‚3ä¸ªè¦ç‚¹ï¼Œå®é™…{list_count}ä¸ª"
    
    elif 'count_5' in prompt_key:
        list_count = count_list_items(answer_text)
        analysis['list_items'] = list_count
        analysis['follows_instruction'] = (list_count == 5)
        analysis['note'] = f"è¦æ±‚5ä¸ªè§‚å¯Ÿç‚¹ï¼Œå®é™…{list_count}ä¸ª"
    
    elif 'one_sentence' in prompt_key:
        sent_count = count_sentences(answer_text)
        analysis['sentence_count'] = sent_count
        analysis['follows_instruction'] = (sent_count == 1)
        analysis['note'] = f"è¦æ±‚1å¥è¯ï¼Œå®é™…{sent_count}å¥"
    
    elif 'focus_people' in prompt_key:
        people_keywords = ['person', 'people', 'man', 'woman', 'he', 'she', 'they', 'facial', 'expression', 'gesture', 'interaction']
        keyword_count = sum(1 for kw in people_keywords if kw in answer_text.lower())
        analysis['people_keyword_count'] = keyword_count
        analysis['follows_instruction'] = (keyword_count >= 5)
        analysis['note'] = f"äººç‰©ç›¸å…³å…³é”®è¯å‡ºç°{keyword_count}æ¬¡"
    
    elif 'focus_environment' in prompt_key:
        env_keywords = ['background', 'setting', 'location', 'environment', 'room', 'outdoor', 'indoor', 'place']
        people_keywords = ['person', 'people', 'man', 'woman']
        env_count = sum(1 for kw in env_keywords if kw in answer_text.lower())
        people_count = sum(1 for kw in people_keywords if kw in answer_text.lower())
        analysis['environment_keyword_count'] = env_count
        analysis['people_keyword_count'] = people_count
        analysis['follows_instruction'] = (env_count > people_count)
        analysis['note'] = f"ç¯å¢ƒè¯{env_count}æ¬¡ vs äººç‰©è¯{people_count}æ¬¡"
    
    elif 'timeline' in prompt_key:
        timeline_keywords = ['first', 'then', 'next', 'after', 'finally', 'initially', 'subsequently']
        keyword_count = sum(1 for kw in timeline_keywords if kw in answer_text.lower())
        analysis['timeline_keyword_count'] = keyword_count
        analysis['follows_instruction'] = (keyword_count >= 3)
        analysis['note'] = f"æ—¶é—´é¡ºåºè¯å‡ºç°{keyword_count}æ¬¡"
    
    else:
        analysis['follows_instruction'] = None
        analysis['note'] = "é€šç”¨å›ç­”ï¼Œæ— ç‰¹å®šè¦æ±‚"
    
    return analysis, extracted


def generate_with_prompt(model, processor, sample, prompt_config, system_prompt):
    """ä½¿ç”¨ç‰¹å®špromptç”Ÿæˆè¾“å‡ºï¼ˆä¸test_base_model.pyä¿æŒä¸€è‡´çš„å¤„ç†æµç¨‹ï¼‰"""
    
    # åˆ›å»ºæ¶ˆæ¯
    messages = create_messages_with_custom_prompt(
        sample, 
        prompt_config['instruction'],
        system_prompt
    )
    
    # åº”ç”¨chat template
    texts = processor.apply_chat_template(
        [messages],
        tokenize=False,
        add_generation_prompt=True
    )
    text = texts[0]
    
    # å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆä¸test_base_modelä¿æŒä¸€è‡´ï¼šuse_audio_in_video=Falseï¼‰
    audios, images, videos = process_mm_info(messages, use_audio_in_video=False)
    
    inputs = processor(
        text=[text],
        images=images,
        videos=videos,
        audio=audios,
        return_tensors="pt",
        padding=True,
        truncation=True,  # ä¸test_base_modelä¿æŒä¸€è‡´
        max_length=32768
    )
    
    inputs = inputs.to(model.device)
    
    # æ£€æŸ¥åºåˆ—é•¿åº¦ï¼ˆé˜²å¾¡æªæ–½ï¼‰
    seq_len = inputs['input_ids'].shape[1]
    if seq_len > 32768:
        raise AssertionError(f"åºåˆ—å¤ªé•¿: {seq_len} > 32768")
    
    # ç”Ÿæˆ
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=False,  # è´ªå©ªè§£ç ï¼Œç¡®ä¿ç»“æœç¨³å®š
            temperature=1.0,
            top_p=0.9
        )
    
    # åªå–ç”Ÿæˆçš„éƒ¨åˆ†
    generated_ids = [
        output_ids[len(input_ids):] 
        for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
    
    generated_text = processor.batch_decode(
        generated_ids, 
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]
    
    return generated_text


def main():
    print("=" * 80)
    print("ğŸ§ª HumanOmniV2 æŒ‡ä»¤éµå¾ªèƒ½åŠ›æµ‹è¯•")
    print("=" * 80)
    print()
    
    # ==================== é…ç½® ====================
    MODEL_PATH = "${PROJECT_ROOT}/models/HumanOmniV2"
    BASE_MODEL_PATH = "${PROJECT_ROOT}/Qwen2.5-Omni-7B-Thinker"
    DATASET_PATH = "../configs/test_samples.yaml"
    
    # ä½¿ç”¨ä¸test_base_modelç›¸åŒçš„system prompt
    SYSTEM_PROMPT = """You are a helpful assistant. Your primary goal is to deeply analyze and interpret information from available various modalities (image, video, audio, text context) to answer questions with human-like depth and a clear, traceable thought process.

Begin by thoroughly understanding the image, video, audio or other available context information, and then proceed with an in-depth analysis related to the question. 

When analyzing videos, YOU MUST reference specific frame numbers and timestamps for key events and observations.
Format: "observation [Frame N: T.XXs]"

Examples of correct temporal references:
- The woman picks up the rose [Frame 3: 3.00s]
- She smiles at the man [Frame 5: 5.00s]
- The man receives the rose [Frame 12: 12.00s]

Pay special attention to the temporal progression of events. Always connect your visual observations to their corresponding frame numbers and timestamps.

In reasoning, It is encouraged to incorporate self-reflection and verification into your reasoning process. You are encouraged to review the image, video, audio, or other context information to ensure the answer accuracy.

Provide your understanding of the image, video, and audio between the <context> </context> tags, detail the reasoning between the <think> </think> tags, and then give your final answer between the <answer> </answer> tags.
"""
    
    # ==================== åŠ è½½æ¨¡å‹ ====================
    print("ğŸ”§ åŠ è½½æ¨¡å‹...")
    print(f"  æ¨¡å‹æƒé‡è·¯å¾„: {MODEL_PATH}")
    
    processor = AutoProcessor.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    
    if hasattr(processor, 'image_processor'):
        processor.image_processor.max_pixels = 6422528
        processor.image_processor.min_pixels = 3136
    
    model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print()
    
    # ==================== åŠ è½½æ•°æ® ====================
    all_samples = load_dataset(DATASET_PATH)
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªè§†é¢‘æ ·æœ¬
    sample = random.choice(all_samples)
    
    print("ğŸ² æµ‹è¯•æ ·æœ¬:")
    print(f"  è§†é¢‘è·¯å¾„: {sample.get('path', 'unknown')}")
    print(f"  æ•°æ®ç±»å‹: {sample.get('data_type', 'unknown')}")
    print()
    
    # ==================== æµ‹è¯•ä¸åŒçš„æŒ‡ä»¤ ====================
    print("=" * 80)
    print("å¼€å§‹æµ‹è¯•ä¸åŒæŒ‡ä»¤...")
    print("=" * 80)
    print()
    
    results = {}
    
    for prompt_key, prompt_config in TEST_PROMPTS.items():
        print(f"\n{'='*80}")
        print(f"ğŸ”¹ æµ‹è¯• {prompt_config['name']}")
        print(f"{'='*80}")
        print(f"æŒ‡ä»¤: {prompt_config['instruction']}")
        print(f"é¢„æœŸ: {prompt_config['expected']}")
        print()
        
        # ç”Ÿæˆè¾“å‡º
        print("â³ ç”Ÿæˆä¸­...")
        generated_text = generate_with_prompt(
            model, processor, sample, prompt_config, SYSTEM_PROMPT
        )
        
        # åˆ†æè¾“å‡º
        prompt_config['key'] = prompt_key
        analysis, extracted = analyze_output(generated_text, prompt_config)
        
        # æ˜¾ç¤ºç»“æœ
        print("âœ… ç”Ÿæˆå®Œæˆ")
        print()
        print("ã€åˆ†æç»“æœã€‘")
        if analysis.get('follows_instruction') is not None:
            status = "âœ… éµå¾ª" if analysis['follows_instruction'] else "âŒ æœªéµå¾ª"
            print(f"  æŒ‡ä»¤éµå¾ª: {status}")
        print(f"  è¯´æ˜: {analysis['note']}")
        
        print()
        print("ã€ç”Ÿæˆçš„ç­”æ¡ˆã€‘")
        answer = extracted.get('answer', 'æ— ')
        print(f"{answer[:500]}{'...' if len(answer) > 500 else ''}")
        print()
        
        # ä¿å­˜ç»“æœ
        results[prompt_key] = {
            "prompt": prompt_config,
            "generated_text": generated_text,
            "extracted": extracted,
            "analysis": analysis
        }
    
    # ==================== æ€»ç»“å¯¹æ¯” ====================
    print("\n" + "=" * 80)
    print("ğŸ“Š æŒ‡ä»¤éµå¾ªèƒ½åŠ›æ€»ç»“")
    print("=" * 80)
    print()
    
    follow_count = 0
    total_testable = 0
    
    print(f"{'æŒ‡ä»¤ç±»å‹':<25} {'éµå¾ªçŠ¶æ€':<15} {'è¯¦ç»†è¯´æ˜'}")
    print("-" * 80)
    
    for prompt_key, result in results.items():
        name = result['prompt']['name']
        follows = result['analysis'].get('follows_instruction')
        note = result['analysis'].get('note', '')
        
        if follows is not None:
            total_testable += 1
            if follows:
                follow_count += 1
                status = "âœ… éµå¾ª"
            else:
                status = "âŒ æœªéµå¾ª"
        else:
            status = "â– æ— æ³•åˆ¤æ–­"
        
        print(f"{name:<25} {status:<15} {note}")
    
    print("-" * 80)
    if total_testable > 0:
        follow_rate = (follow_count / total_testable) * 100
        print(f"\nğŸ“ˆ æŒ‡ä»¤éµå¾ªç‡: {follow_count}/{total_testable} ({follow_rate:.1f}%)")
    
    # ==================== ä¿å­˜ç»“æœ ====================
    log_dir = Path("../logs")
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = log_dir / f"instruction_following_test_{timestamp}.json"
    
    test_result = {
        "timestamp": timestamp,
        "model_path": MODEL_PATH,
        "video_path": sample.get('path', 'unknown'),
        "results": results,
        "summary": {
            "total_prompts": len(TEST_PROMPTS),
            "testable_prompts": total_testable,
            "followed_prompts": follow_count,
            "follow_rate": f"{follow_rate:.1f}%" if total_testable > 0 else "N/A"
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(test_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ==================== ç»“è®º ====================
    print("\n" + "=" * 80)
    print("ğŸ¯ æµ‹è¯•ç»“è®º")
    print("=" * 80)
    print()
    
    if total_testable == 0:
        print("âš ï¸  æ— æ³•è¯„ä¼°æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ˆæ‰€æœ‰æµ‹è¯•éƒ½æ— æ³•åˆ¤æ–­ï¼‰")
    elif follow_rate >= 80:
        print("âœ… æŒ‡ä»¤éµå¾ªèƒ½åŠ› **å¼º**")
        print("   æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°ç†è§£å’Œæ‰§è¡Œä¸åŒç±»å‹çš„æŒ‡ä»¤")
    elif follow_rate >= 50:
        print("âš ï¸  æŒ‡ä»¤éµå¾ªèƒ½åŠ› **ä¸­ç­‰**")
        print("   æ¨¡å‹èƒ½ç†è§£éƒ¨åˆ†æŒ‡ä»¤ï¼Œä½†æ‰§è¡Œä¸å¤Ÿç²¾ç¡®")
    else:
        print("âŒ æŒ‡ä»¤éµå¾ªèƒ½åŠ› **å¼±**")
        print("   æ¨¡å‹éš¾ä»¥éµå¾ªå…·ä½“çš„æŒ‡ä»¤è¦æ±‚")
        print("   å¯èƒ½è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šçš„é—®ç­”æ ¼å¼")
    
    print()


if __name__ == "__main__":
    main()
