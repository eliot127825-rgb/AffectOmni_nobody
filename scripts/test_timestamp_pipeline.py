"""
å®Œæ•´çš„æ—¶é—´æˆ³åå¤„ç† Pipeline æµ‹è¯•è„šæœ¬
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼šè§†é¢‘é‡‡å¸§ + æ¨¡å‹æ¨ç† + äº‹ä»¶æå– + CLIP åŒ¹é… + æ—¶é—´æˆ³æ’å…¥
"""

import sys
import os
import random
import yaml
import json
import torch
from datetime import datetime
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../tools'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../src/src'))

from transformers import AutoProcessor, Qwen2_5OmniThinkerForConditionalGeneration

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from tools.video_utils import sample_frames, get_video_info
from tools.clip_matcher import CLIPMatcher, match_with_monotonic_constraint
from extract_events import extract_events, events_to_queries
from insert_timestamps import insert_timestamps, verify_insertions


def load_test_samples(dataset_path: str):
    """åŠ è½½æµ‹è¯•æ ·æœ¬ï¼ˆä¸test_base_model.pyä¸€è‡´ï¼‰"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {dataset_path}")
    
    with open(dataset_path, 'r') as f:
        yaml_data = yaml.safe_load(f)
    
    datasets = yaml_data.get('datasets', [])
    all_samples = []
    
    for dataset_config in datasets:
        json_path = dataset_config.get('json_path')
        data_root = dataset_config.get('data_root')
        
        print(f"  â”œâ”€ åŠ è½½: {json_path}")
        
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # æ·»åŠ data_rootè·¯å¾„
        if data_root:
            for sample in data:
                if 'path' in sample:
                    sample['path'] = os.path.join(data_root, sample['path'])
        
        all_samples.extend(data)
        print(f"  â””â”€ åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")
    
    print(f"\nâœ… æ€»å…±åŠ è½½ {len(all_samples)} ä¸ªæ ·æœ¬\n")
    return all_samples


def format_question(sample):
    """æ ¼å¼åŒ–é—®é¢˜ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€æ ·ï¼‰"""
    if sample['problem_type'] in ['multiple choice', 'emer_ov_mc']:
        question = sample['problem'] + "\nOptions:\n"
        for option in sample.get('options', []):
            question += option + "\n"
    else:
        question = sample['problem']
    
    return question


def create_messages(sample, system_prompt, timestamp_info=None):
    """åˆ›å»ºå¯¹è¯æ¶ˆæ¯ï¼ˆå’Œtest_base_model.pyä¸€è‡´ï¼‰"""
    question = format_question(sample)
    
    # TYPE_TEMPLATE
    TYPE_TEMPLATES = {
        "multiple choice": " Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags.",
        "numerical": " Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags.",
        "OCR": " Please transcribe text from the image/video clearly and provide your text answer within the <answer> </answer> tags.",
        "free-form": " Please provide your text answer within the <answer> </answer> tags.",
        "regression": " Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags.",
        "emer_ov": " Please provide the words to describe emotions within the  <answer> </answer> tags.",
        "emer_ov_mc": " Please provide only the single or multiple option letter (e.g., A for single option or A,E for multi option, etc.) within the <answer> </answer> tags.",
    }
    
    text_prompt = question + "\n" + TYPE_TEMPLATES.get(sample['problem_type'], "")
    
    # å¦‚æœæä¾›äº†æ—¶é—´æˆ³ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
    if timestamp_info:
        text_prompt += "\n\n" + timestamp_info
    
    # æ„é€ messages
    messages = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": system_prompt
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": sample.get('data_type', 'video'),
                    sample.get('data_type', 'video'): sample['path'],
                    "max_frames": 32,  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
                    "max_pixels": 602112
                },
                {
                    "type": "text",
                    "text": text_prompt
                }
            ]
        }
    ]
    
    return messages


def run_inference(sample, model, processor, system_prompt):
    """è¿è¡Œæ¨¡å‹æ¨ç†ï¼ˆä¸å¸¦æ—¶é—´æˆ³ï¼Œä¸test_base_model.pyä¸€è‡´ï¼‰"""
    from qwen_omni_utils import process_mm_info
    
    # æ„é€ æ¶ˆæ¯ï¼ˆä¸å¸¦æ—¶é—´æˆ³ï¼‰
    messages = create_messages(sample, system_prompt, timestamp_info=None)
    
    # åº”ç”¨ chat template
    texts = processor.apply_chat_template(
        [messages],
        tokenize=False,
        add_generation_prompt=True
    )
    text = texts[0]
    
    # å¤„ç†å¤šæ¨¡æ€
    audios, images, videos = process_mm_info(messages, use_audio_in_video=False)
    
    # å¤„ç†è¾“å…¥
    inputs = processor(
        text=[text],
        images=images,
        videos=videos,
        audio=audios,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=32768
    ).to(model.device)
    
    # ç”Ÿæˆï¼ˆä¸test_base_model.pyå‚æ•°ä¸€è‡´ï¼‰
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )
    
    # è§£ç 
    generated_text = processor.batch_decode(
        outputs[:, inputs['input_ids'].shape[1]:],
        skip_special_tokens=True
    )[0]
    
    return generated_text, inputs


def parse_think_section(text: str) -> str:
    """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå– <think> éƒ¨åˆ†"""
    import re
    match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return ""


def main():
    print("=" * 80)
    print("ğŸ¬ æ—¶é—´æˆ³åå¤„ç† Pipeline æµ‹è¯•")
    print("=" * 80)
    print()
    
    # ==================== é…ç½® ====================
    MODEL_PATH = "${PROJECT_ROOT}/models/HumanOmniV2"
    BASE_MODEL_PATH = "${PROJECT_ROOT}/Qwen2.5-Omni-7B-Thinker"
    DATASET_PATH = "../configs/test_samples.yaml"
    MAX_FRAMES = 16  # å‡å°‘å¸§æ•°ä»¥åŠ å¿«é€Ÿåº¦
    CLIP_MODEL = "ViT-B-32"
    USE_MONOTONIC_CONSTRAINT = True  # ä¿æŒæ—¶åºçº¦æŸ
    LAMBDA_SMOOTH = 0.01  # æå°çš„å¹³æ»‘çº¦æŸï¼Œä¸»è¦ä¾èµ–CLIPç›¸ä¼¼åº¦
    
    SYSTEM_PROMPT = """You are a helpful assistant. Your primary goal is to deeply analyze and interpret information from available various modalities (image, video, audio, text context) to answer questions with human-like depth and a clear, traceable thought process.

Begin by thoroughly understanding the image, video, audio or other available context information, and then proceed with an in-depth analysis related to the question.

Provide your understanding of the image, video, and audio between the <context> </context> tags, detail the reasoning between the <think> </think> tags, and then give your final answer between the <answer> </answer> tags."""
    
    # ==================== åŠ è½½æ¨¡å‹ ====================
    print("ğŸ”§ åŠ è½½æ¨¡å‹...")
    print(f"  æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"  Processorè·¯å¾„: {BASE_MODEL_PATH}")
    
    # Processor ä»åŸºåº§æ¨¡å‹åŠ è½½
    processor = AutoProcessor.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    
    # æ¨¡å‹ä½¿ç”¨è‡ªå®šä¹‰æ¶æ„åŠ è½½
    model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {model.device})")
    print()
    
    # ==================== åŠ è½½ CLIP ====================
    print(f"ğŸ”§ åŠ è½½ CLIP æ¨¡å‹: {CLIP_MODEL}...")
    # ä½¿ç”¨åŸç‰ˆ CLIPï¼ˆç¦»çº¿å‹å¥½ï¼Œä¸éœ€è¦ä» HuggingFace ä¸‹è½½ï¼‰
    clip_matcher = CLIPMatcher(model_name=CLIP_MODEL, device="cuda", use_original_clip=True)
    print()
    
    # ==================== åŠ è½½æµ‹è¯•æ ·æœ¬ ====================
    print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {DATASET_PATH}")
    all_samples = load_test_samples(DATASET_PATH)
    sample = random.choice(all_samples)
    
    print(f"âœ… éšæœºæŠ½å–æ ·æœ¬:")
    print(f"  è§†é¢‘è·¯å¾„: {sample['path']}")
    print(f"  é—®é¢˜: {sample['problem'][:100]}...")
    print()
    
    # ==================== é˜¶æ®µ1: é‡‡æ ·è§†é¢‘å¸§ ====================
    print("=" * 80)
    print("ğŸ“¹ é˜¶æ®µ1: é‡‡æ ·è§†é¢‘å¸§")
    print("=" * 80)
    
    video_path = sample['path']
    video_info = get_video_info(video_path)
    print(f"è§†é¢‘ä¿¡æ¯:")
    print(f"  æ€»å¸§æ•°: {video_info['total_frames']}")
    print(f"  FPS: {video_info['fps']:.2f}")
    print(f"  æ€»æ—¶é•¿: {video_info['duration']:.2f}ç§’")
    
    frames_pil, frame_ids, timestamps, fps = sample_frames(
        video_path, max_frames=MAX_FRAMES, strategy="uniform"
    )
    print(f"âœ… é‡‡æ ·å®Œæˆ: {len(frames_pil)} å¸§")
    print(f"  å¸§å·èŒƒå›´: {frame_ids[0]} ~ {frame_ids[-1]}")
    print(f"  æ—¶é—´èŒƒå›´: {timestamps[0]:.2f}s ~ {timestamps[-1]:.2f}s")
    print()
    
    # ==================== é˜¶æ®µ2: æ¨¡å‹æ¨ç† ====================
    print("=" * 80)
    print("ğŸ¤– é˜¶æ®µ2: æ¨¡å‹æ¨ç†ï¼ˆä¸å¸¦æ—¶é—´æˆ³ï¼‰")
    print("=" * 80)
    
    generated_text, inputs = run_inference(sample, model, processor, SYSTEM_PROMPT)
    think_text = parse_think_section(generated_text)
    
    print("âœ… æ¨ç†å®Œæˆ")
    print(f"ç”Ÿæˆé•¿åº¦: {len(generated_text)} å­—ç¬¦")
    print(f"\nã€åŸå§‹ <think> å†…å®¹ã€‘")
    print("-" * 80)
    print(think_text[:500] + "..." if len(think_text) > 500 else think_text)
    print("-" * 80)
    print()
    
    # ==================== é˜¶æ®µ3: æå–äº‹ä»¶ ====================
    print("=" * 80)
    print("ğŸ” é˜¶æ®µ3: æå–å…³é”®äº‹ä»¶")
    print("=" * 80)
    
    # å°è¯• LLM æ–¹æ³•
    print("å°è¯•ä½¿ç”¨ LLM æå–äº‹ä»¶...")
    events = extract_events(
        think_text,
        method="llm",
        model=model,
        processor=processor,
        max_events=10
    )
    
    if not events:
        print("âš ï¸  LLM æå–å¤±è´¥ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•")
        events = extract_events(think_text, method="rule", max_events=10)
    
    print(f"âœ… æå–åˆ° {len(events)} ä¸ªäº‹ä»¶:")
    for i, event in enumerate(events, 1):
        print(f"  {i}. anchor: {event.anchor[:60]}...")
        print(f"     query:  {event.query}")
    print()
    
    # ==================== é˜¶æ®µ4: CLIP åŒ¹é… ====================
    print("=" * 80)
    print("ğŸ¯ é˜¶æ®µ4: CLIP äº‹ä»¶-å¸§åŒ¹é…")
    print("=" * 80)
    
    queries = events_to_queries(events)
    
    if USE_MONOTONIC_CONSTRAINT:
        print("ä½¿ç”¨å•è°ƒçº¦æŸ DP...")
        similarity_matrix = clip_matcher.get_similarity_matrix(queries, frames_pil)
        best_frames = match_with_monotonic_constraint(
            similarity_matrix,
            lambda_smooth=LAMBDA_SMOOTH
        )
        frame_matches = {q: f for q, f in zip(queries, best_frames)}
    else:
        print("ä½¿ç”¨ç‹¬ç«‹åŒ¹é…...")
        frame_matches = clip_matcher.match_events_to_frames(queries, frames_pil)
    
    print(f"âœ… åŒ¹é…å®Œæˆ:")
    for i, (event, frame_id) in enumerate(zip(events, best_frames if USE_MONOTONIC_CONSTRAINT else [frame_matches[q] for q in queries]), 1):
        timestamp = timestamps[frame_id]
        print(f"  {i}. {event.query[:40]:<40} â†’ Frame {frame_id:2d} ({timestamp:5.2f}s)")
    print()
    
    # ==================== é˜¶æ®µ5: æ’å…¥æ—¶é—´æˆ³ ====================
    print("=" * 80)
    print("âœï¸  é˜¶æ®µ5: æ’å…¥æ—¶é—´æˆ³")
    print("=" * 80)
    
    think_with_timestamps = insert_timestamps(
        think_text,
        events,
        frame_matches,
        timestamps,
        format_style="frame_and_time"
    )
    
    # éªŒè¯æ’å…¥ç»“æœ
    verification = verify_insertions(think_text, think_with_timestamps, len(events))
    print(f"âœ… æ’å…¥å®Œæˆ:")
    print(f"  é¢„æœŸæ’å…¥: {verification['expected_count']} ä¸ª")
    print(f"  å®é™…æ’å…¥: {verification['inserted_count']} ä¸ª")
    print(f"  æ’å…¥ç‡: {verification['insertion_rate']:.1%}")
    print()
    
    print(f"ã€å¸¦æ—¶é—´æˆ³çš„ <think> å†…å®¹ã€‘")
    print("=" * 80)
    print(think_with_timestamps)
    print("=" * 80)
    print()
    
    # ==================== ä¿å­˜ç»“æœ ====================
    log_dir = Path("../logs")
    log_dir.mkdir(exist_ok=True)
    
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = log_dir / f"timestamp_pipeline_{timestamp_str}.json"
    
    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_to_native(obj):
        """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
        import numpy as np
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_native(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_native(item) for item in obj]
        else:
            return obj
    
    result = {
        "video_path": video_path,
        "question": sample['problem'],
        "video_info": convert_to_native(video_info),
        "num_frames_sampled": len(frames_pil),
        "num_events_extracted": len(events),
        "events": [e.to_dict() for e in events],
        "frame_matches": {e.query: int(frame_matches[e.query]) for e in events},
        "original_think": think_text,
        "think_with_timestamps": think_with_timestamps,
        "verification": verification,
        "config": {
            "max_frames": MAX_FRAMES,
            "clip_model": CLIP_MODEL,
            "use_monotonic_constraint": USE_MONOTONIC_CONSTRAINT,
            "lambda_smooth": LAMBDA_SMOOTH
        }
    }
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    print()
    
    # ==================== æ€»ç»“ ====================
    print("=" * 80)
    print("âœ… Pipeline æ‰§è¡Œå®Œæˆ")
    print("=" * 80)
    print(f"âœ“ é‡‡æ ·å¸§æ•°: {len(frames_pil)}")
    print(f"âœ“ æå–äº‹ä»¶: {len(events)}")
    print(f"âœ“ æ—¶é—´æˆ³æ’å…¥ç‡: {verification['insertion_rate']:.1%}")
    print()
    print("ğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("  1. æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åˆç†ï¼ˆæ˜¯å¦ç¬¦åˆè§†é¢‘å†…å®¹ï¼‰")
    print("  2. è°ƒæ•´å‚æ•°ï¼ˆmax_frames, lambda_smoothï¼‰ä¼˜åŒ–æ•ˆæœ")
    print("  3. åœ¨æ›´å¤šæ ·æœ¬ä¸Šæµ‹è¯•æ³›åŒ–æ€§èƒ½")
    print()


if __name__ == "__main__":
    main()
