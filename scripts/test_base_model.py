#!/usr/bin/env python3
"""
ç®€å•çš„Stage 1æ¨¡å‹è¯„ä¼°è„šæœ¬
éšæœºæŠ½å–è®­ç»ƒæ ·æœ¬ï¼Œæµ‹è¯•æ¨¡å‹è¾“å‡º
"""

import os
import sys
import json
import yaml
import random
import torch
import re
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcè·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src" / "src"))

from transformers import AutoProcessor, Qwen2_5OmniThinkerForConditionalGeneration
from open_r1.vlm_modules.qwenomni_module import QwenOmniModule
from qwen_omni_utils import process_mm_info


# è®¾ç½®éšæœºç§å­
random.seed(42)
torch.manual_seed(42)


def load_dataset(yaml_path):
    """åŠ è½½è®­ç»ƒæ•°æ®é›†"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {yaml_path}")
    
    with open(yaml_path, 'r') as f:
        yaml_data = yaml.safe_load(f)
    
    datasets = yaml_data.get('datasets', [])
    all_samples = []
    
    for dataset_config in datasets:
        json_path = dataset_config.get('json_path')
        data_root = dataset_config.get('data_root')
        
        print(f"  â”œâ”€ åŠ è½½: {json_path}")
        
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # æ·»åŠ data_rootè·¯å¾„
        if data_root:
            for sample in data:
                if 'path' in sample:
                    sample['path'] = os.path.join(data_root, sample['path'])
        
        all_samples.extend(data)
        print(f"  â””â”€ åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")
    
    print(f"\nâœ… æ€»å…±åŠ è½½ {len(all_samples)} ä¸ªæ ·æœ¬\n")
    return all_samples


def format_question(sample):
    """æ ¼å¼åŒ–é—®é¢˜ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€æ ·ï¼‰"""
    if sample['problem_type'] in ['multiple choice', 'emer_ov_mc']:
        question = sample['problem'] + "\nOptions:\n"
        for option in sample.get('options', []):
            question += option + "\n"
    else:
        question = sample['problem']
    
    return question


def create_messages(sample, system_prompt, timestamp_info=None):
    """åˆ›å»ºå¯¹è¯æ¶ˆæ¯ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€æ ·çš„æ ¼å¼ï¼‰
    
    Args:
        sample: æ ·æœ¬æ•°æ®
        system_prompt: ç³»ç»Ÿæç¤º
        timestamp_info: å¯é€‰çš„æ—¶é—´æˆ³ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œå¦‚æœæä¾›åˆ™æ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
    """
    question = format_question(sample)
    
    # TYPE_TEMPLATE
    TYPE_TEMPLATES = {
        "multiple choice": " Please provide only the single option letter (e.g., A, B, C, D, etc.) within the <answer> </answer> tags.",
        "numerical": " Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags.",
        "OCR": " Please transcribe text from the image/video clearly and provide your text answer within the <answer> </answer> tags.",
        "free-form": " Please provide your text answer within the <answer> </answer> tags.",
        "regression": " Please provide the numerical value (e.g., 42 or 3.14) within the <answer> </answer> tags.",
        "emer_ov": " Please provide the words to describe emotions within the  <answer> </answer> tags.",
        "emer_ov_mc": " Please provide only the single or multiple option letter (e.g., A for single option or A,E for multi option, etc.) within the <answer> </answer> tags.",
    }
    
    text_prompt = question + "\n" + TYPE_TEMPLATES.get(sample['problem_type'], "")
    
    # å¦‚æœæä¾›äº†æ—¶é—´æˆ³ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
    if timestamp_info:
        text_prompt += "\n\n" + timestamp_info
    
    # æ„é€ messages
    messages = [
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": system_prompt
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": sample.get('data_type', 'video'),
                    sample.get('data_type', 'video'): sample['path'],
                    "max_frames": 32,  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
                    "max_pixels": 602112
                },
                {
                    "type": "text",
                    "text": text_prompt
                }
            ]
        }
    ]
    
    return messages


def extract_tags(text):
    """æå–<context>, <think>, <answer>æ ‡ç­¾å†…å®¹"""
    context_match = re.search(r'<context>(.*?)</context>', text, re.DOTALL)
    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)
    answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)
    
    return {
        'context': context_match.group(1).strip() if context_match else None,
        'think': think_match.group(1).strip() if think_match else None,
        'answer': answer_match.group(1).strip() if answer_match else None
    }


def check_format(generated_text):
    """æ£€æŸ¥è¾“å‡ºæ ¼å¼æ˜¯å¦æ­£ç¡®"""
    has_context = '<context>' in generated_text and '</context>' in generated_text
    has_think = '<think>' in generated_text and '</think>' in generated_text
    has_answer = '<answer>' in generated_text and '</answer>' in generated_text
    
    return {
        'has_context': has_context,
        'has_think': has_think,
        'has_answer': has_answer,
        'all_correct': has_context and has_think and has_answer
    }


def main():
    print("=" * 80)
    print("ğŸ§ª HumanOmniV2 åŸºåº§æ¨¡å‹æµ‹è¯•è„šæœ¬")
    print("=" * 80)
    print()
    
    # ==================== é…ç½® ====================
    MODEL_PATH = "${PROJECT_ROOT}/models/HumanOmniV2"  # è®­ç»ƒå¥½çš„HumanOmniV2æ¨¡å‹
    BASE_MODEL_PATH = "${PROJECT_ROOT}/Qwen2.5-Omni-7B-Thinker"  # åŸºåº§æ¨¡å‹ï¼ˆç”¨äºåŠ è½½processorï¼‰
    DATASET_PATH = "../configs/test_samples.yaml"  # æµ‹è¯•æ•°æ®é›†é…ç½®
    
    SYSTEM_PROMPT = """You are a helpful assistant. Your primary goal is to deeply analyze and interpret information from available various modalities (image, video, audio, text context) to answer questions with human-like depth and a clear, traceable thought process.

Begin by thoroughly understanding the image, video, audio or other available context information, and then proceed with an in-depth analysis related to the question. 

When analyzing videos, YOU MUST reference specific frame numbers and timestamps for key events and observations.
Format: "observation [Frame N: T.XXs]"

Examples of correct temporal references:
- The woman picks up the rose [Frame 3: 3.00s]
- She smiles at the man [Frame 5: 5.00s]
- The man receives the rose [Frame 12: 12.00s]

Pay special attention to the temporal progression of events. Always connect your visual observations to their corresponding frame numbers and timestamps.

In reasoning, It is encouraged to incorporate self-reflection and verification into your reasoning process. You are encouraged to review the image, video, audio, or other context information to ensure the answer accuracy.

Provide your understanding of the image, video, and audio between the <context> </context> tags, detail the reasoning between the <think> </think> tags, and then give your final answer between the <answer> </answer> tags.
"""
    
    # ==================== åŠ è½½æ¨¡å‹ ====================
    print("ğŸ”§ åŠ è½½æ¨¡å‹...")
    print(f"  æ¨¡å‹æƒé‡è·¯å¾„: {MODEL_PATH}")
    print(f"  Processorè·¯å¾„: {BASE_MODEL_PATH}")
    
    # Processorä»åŸºåº§æ¨¡å‹åŠ è½½ï¼ˆå› ä¸ºè®­ç»ƒæ—¶åªä¿å­˜äº†æ¨¡å‹æƒé‡ï¼‰
    processor = AutoProcessor.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    
    # è¦†ç›–å…¨å±€é…ç½®ï¼ˆé˜²å¾¡æªæ–½3ï¼‰
    if hasattr(processor, 'image_processor'):
        processor.image_processor.max_pixels = 6422528
        processor.image_processor.min_pixels = 3136
    
    # æ¨¡å‹æƒé‡ä»è®­ç»ƒè¾“å‡ºåŠ è½½
    model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print(f"  è®¾å¤‡: {model.device}")
    print()
    
    # ==================== åŠ è½½æ•°æ® ====================
    all_samples = load_dataset(DATASET_PATH)
    
    # ==================== éšæœºæŠ½å–æ ·æœ¬ ====================
    sample = random.choice(all_samples)
    
    print("ğŸ² éšæœºæŠ½å–çš„æ ·æœ¬:")
    print(f"  é—®é¢˜ç±»å‹: {sample.get('problem_type', 'unknown')}")
    print(f"  æ•°æ®ç±»å‹: {sample.get('data_type', 'unknown')}")
    print(f"  æ–‡ä»¶è·¯å¾„: {sample.get('path', 'unknown')}")
    print(f"  é—®é¢˜: {format_question(sample)[:200]}...")
    print()
    
    # ==================== ç¬¬ä¸€æ­¥ï¼šå…ˆå¤„ç†è§†é¢‘è·å–æ—¶é—´æˆ³ä¿¡æ¯ ====================
    print("ğŸ“ ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†è§†é¢‘è·å–æ—¶é—´æˆ³...")
    
    # å…ˆç”¨ä¸åŒ…å«æ—¶é—´æˆ³çš„messageså¤„ç†ä¸€æ¬¡ï¼Œè·å–å®é™…çš„å¸§æ•°å’Œæ—¶é—´é—´éš”
    temp_messages = create_messages(sample, SYSTEM_PROMPT, timestamp_info=None)
    temp_texts = processor.apply_chat_template(
        [temp_messages],
        tokenize=False,
        add_generation_prompt=True
    )
    temp_text = temp_texts[0]
    
    # å¤„ç†å¤šæ¨¡æ€è¾“å…¥
    audios, images, videos = process_mm_info(temp_messages, use_audio_in_video=False)
    
    # ä¸´æ—¶å¤„ç†è·å–æ—¶é—´æˆ³ä¿¡æ¯
    temp_inputs = processor(
        text=[temp_text],
        images=images,
        videos=videos,
        audio=audios,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=32768
    )
    
    # æå–æ—¶é—´æˆ³ä¿¡æ¯
    timestamp_info_str = None
    if 'video_grid_thw' in temp_inputs and temp_inputs['video_grid_thw'] is not None:
        video_grid = temp_inputs['video_grid_thw']
        num_frames = video_grid[0][0].item()
        
        if 'video_second_per_grid' in temp_inputs and temp_inputs['video_second_per_grid'] is not None:
            second_per_grid = temp_inputs['video_second_per_grid']
            
            # è·å–æ—¶é—´é—´éš”
            if second_per_grid.dim() == 0:
                interval = second_per_grid.item()
            elif second_per_grid.dim() == 1 and len(second_per_grid) == 1:
                interval = second_per_grid[0].item()
            else:
                interval = second_per_grid.flatten()[0].item()
            
            # è®¡ç®—æ¯å¸§çš„æ—¶é—´æˆ³
            frame_timestamps = [i * interval for i in range(num_frames)]
            
            # æ„é€ æ—¶é—´æˆ³ä¿¡æ¯å­—ç¬¦ä¸²
            timestamp_info_str = "[Video Frame Information]\n"
            timestamp_info_str += f"This video has been sampled into {num_frames} frames at {interval:.2f}-second intervals.\n"
            timestamp_info_str += "Available frame timestamps:\n"
            timestamp_info_str += ", ".join([f"Frame {i}: {ts:.2f}s" for i, ts in enumerate(frame_timestamps)])
            timestamp_info_str += "\n\n"
            timestamp_info_str += "IMPORTANT: In your <think> section, you MUST reference specific frame numbers for each key event or observation.\n"
            timestamp_info_str += "Use the exact format: \"your observation [Frame N: T.XXs]\"\n"
            timestamp_info_str += "Example: The woman smiles [Frame 5: 5.00s], indicating happiness."
            
            print(f"  âœ… æå–åˆ° {num_frames} å¸§ï¼Œæ—¶é—´é—´éš” {interval:.2f}ç§’/å¸§")
    
    # ==================== ç¬¬äºŒæ­¥ï¼šç”¨æ—¶é—´æˆ³ä¿¡æ¯é‡æ–°æ„é€ å®Œæ•´è¾“å…¥ ====================
    print("ğŸ“ ç¬¬äºŒæ­¥ï¼šæ„é€ åŒ…å«æ—¶é—´æˆ³çš„å®Œæ•´è¾“å…¥...")
    messages = create_messages(sample, SYSTEM_PROMPT, timestamp_info=timestamp_info_str)
    
    # åº”ç”¨chat template
    texts = processor.apply_chat_template(
        [messages],
        tokenize=False,
        add_generation_prompt=True
    )
    text = texts[0]
    
    # é‡æ–°å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆä½¿ç”¨ç›¸åŒçš„æ•°æ®ï¼‰
    audios, images, videos = process_mm_info(messages, use_audio_in_video=False)
    
    # è°ƒè¯•ï¼šæ˜¾ç¤ºæå–çš„å¤šæ¨¡æ€æ•°æ®
    print(f"  å¤šæ¨¡æ€æ•°æ®æå–:")
    print(f"     - éŸ³é¢‘æ•°æ®: {len(audios) if audios else 0} ä¸ª")
    print(f"     - å›¾åƒæ•°æ®: {len(images) if images else 0} ä¸ª")
    print(f"     - è§†é¢‘æ•°æ®: {len(videos) if videos else 0} ä¸ª")
    
    # è¯»å–è§†é¢‘çš„å®é™…æ€»æ—¶é•¿
    video_duration = None
    if videos and len(videos) > 0:
        video_path = sample['path']
        try:
            import cv2
            cap = cv2.VideoCapture(video_path)
            if cap.isOpened():
                fps = cap.get(cv2.CAP_PROP_FPS)
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                if fps > 0:
                    video_duration = frame_count / fps
                cap.release()
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è¯»å–è§†é¢‘æ—¶é•¿: {e}")
    
    inputs = processor(
        text=[text],
        images=images,
        videos=videos,
        audio=audios,
        return_tensors="pt",
        padding=True,
        truncation=True,  # é˜²å¾¡æªæ–½2ï¼štruncationä¿æŠ¤
        max_length=32768
    )
    
    inputs = inputs.to(model.device)
    
    # é˜²å¾¡æªæ–½1ï¼šå®æµ‹æ–­è¨€ + è°ƒè¯•ä¿¡æ¯
    seq_len = inputs['input_ids'].shape[1]
    print(f"âœ… è¾“å…¥å‡†å¤‡å®Œæˆ")
    print(f"  è¾“å…¥tokenæ•°: {seq_len}")
    
    # è°ƒè¯•ï¼šæ‰“å°è§†é¢‘åƒç´ æ•°æ®çš„å®é™…å¤§å°
    if 'pixel_values_videos' in inputs and inputs['pixel_values_videos'] is not None:
        vid_shape = inputs['pixel_values_videos'].shape
        vid_size_gb = inputs['pixel_values_videos'].element_size() * inputs['pixel_values_videos'].nelement() / (1024**3)
        print(f"  è§†é¢‘åƒç´ æ•°æ®shape: {vid_shape}")
        print(f"  è§†é¢‘åƒç´ æ•°æ®å¤§å°: {vid_size_gb:.2f} GB")
    
    # æ‰“å°è§†é¢‘å¸§æ•°å’Œæ—¶é—´æˆ³ä¿¡æ¯
    if 'video_grid_thw' in inputs and inputs['video_grid_thw'] is not None:
        video_grid = inputs['video_grid_thw']
        num_frames = video_grid[0][0].item()  # Tç»´åº¦å°±æ˜¯å¸§æ•°
        print(f"  ğŸ“¹ è§†é¢‘åˆ†æä¿¡æ¯:")
        if video_duration is not None:
            print(f"     - è§†é¢‘æ€»æ—¶é•¿: {video_duration:.2f}ç§’")
        print(f"     - é‡‡æ ·å¸§æ•°: {num_frames} å¸§")
        print(f"     - ç½‘æ ¼ç»´åº¦ (TÃ—HÃ—W): {video_grid[0][0].item()}Ã—{video_grid[0][1].item()}Ã—{video_grid[0][2].item()}")
        
        # æ‰“å°æ¯å¸§çš„æ—¶é—´æˆ³
        if 'video_second_per_grid' in inputs and inputs['video_second_per_grid'] is not None:
            second_per_grid = inputs['video_second_per_grid']
            
            # video_second_per_grid æ˜¯æ¯ä¸ªæ—¶é—´ç½‘æ ¼çš„ç§’æ•°ï¼ˆé—´éš”ï¼‰ï¼Œä¸æ˜¯æ—¶é—´æˆ³åˆ—è¡¨
            if second_per_grid.dim() == 0:
                interval = second_per_grid.item()
            elif second_per_grid.dim() == 1 and len(second_per_grid) == 1:
                interval = second_per_grid[0].item()
            else:
                # å¦‚æœæ˜¯å¤šä¸ªå€¼ï¼Œå–ç¬¬ä¸€ä¸ª
                interval = second_per_grid.flatten()[0].item()
            
            # æ ¹æ®å¸§æ•°å’Œæ—¶é—´é—´éš”è®¡ç®—æ¯å¸§çš„æ—¶é—´æˆ³
            frame_timestamps = [i * interval for i in range(num_frames)]
            
            print(f"     - æ—¶é—´é—´éš”: {interval:.2f}ç§’/å¸§")
            print(f"     - é‡‡æ ·è¦†ç›–èŒƒå›´: {frame_timestamps[0]:.2f}ç§’ ~ {frame_timestamps[-1]:.2f}ç§’")
            print(f"     - é‡‡æ ·è·¨åº¦: {frame_timestamps[-1] - frame_timestamps[0]:.2f}ç§’")
            
            # æ˜¾ç¤ºæ‰€æœ‰å¸§çš„æ—¶é—´æˆ³
            timestamps_str = [f'{t:.2f}s' for t in frame_timestamps]
            print(f"     - å„å¸§æ—¶é—´æˆ³ ({num_frames}å¸§): {timestamps_str}")
    
    # æ‰“å°éŸ³é¢‘ä¿¡æ¯
    if 'input_features' in inputs and inputs['input_features'] is not None:
        audio_features = inputs['input_features']
        print(f"  ğŸµ éŸ³é¢‘åˆ†æä¿¡æ¯:")
        print(f"     - éŸ³é¢‘ç‰¹å¾shape: {audio_features.shape}")
        
        if 'audio_feature_lengths' in inputs and inputs['audio_feature_lengths'] is not None:
            audio_lengths = inputs['audio_feature_lengths']
            print(f"     - éŸ³é¢‘ç‰¹å¾é•¿åº¦: {audio_lengths}")
            # éŸ³é¢‘é‡‡æ ·ç‡é€šå¸¸æ˜¯16kHzï¼Œæ¯ä¸ªç‰¹å¾å¯¹åº”ä¸€å®šæ—¶é•¿
            # Qwen2.5-Omniçš„éŸ³é¢‘å¤„ç†ï¼šæ¯ç§’çº¦50ä¸ªç‰¹å¾å¸§
            if audio_lengths.numel() > 0:
                total_audio_frames = audio_lengths[0].item() if audio_lengths.dim() > 0 else audio_lengths.item()
                # å‡è®¾æ¯ç§’50ä¸ªéŸ³é¢‘ç‰¹å¾å¸§ï¼ˆè¿™æ˜¯Whisperç­‰æ¨¡å‹çš„å¸¸è§è®¾ç½®ï¼‰
                audio_duration_estimate = total_audio_frames / 50.0
                print(f"     - éŸ³é¢‘æ—¶é•¿ä¼°è®¡: {audio_duration_estimate:.2f}ç§’ (åŸºäºç‰¹å¾å¸§æ•°)")
    
    if seq_len > 32768:
        raise AssertionError(f"åºåˆ—å¤ªé•¿: {seq_len} > 32768")
    print()
    
    # ==================== ç”Ÿæˆè¾“å‡º ====================
    print("ğŸ¤– å¼€å§‹ç”Ÿæˆè¾“å‡º...")
    print("-" * 80)
    
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=False,  # è´ªå©ªè§£ç ï¼Œç¡®ä¿ç»“æœç¨³å®š
            temperature=1.0,
            top_p=0.9
        )
    
    # åªå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥ï¼‰
    generated_ids = [
        output_ids[len(input_ids):] 
        for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
    
    generated_text = processor.batch_decode(
        generated_ids, 
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]
    
    print("ç”Ÿæˆå®Œæˆï¼")
    print("-" * 80)
    print()
    
    # ==================== åˆ†æè¾“å‡º ====================
    print("ğŸ“Š è¾“å‡ºåˆ†æ:")
    print("=" * 80)
    
    # 1. æ ¼å¼æ£€æŸ¥
    format_check = check_format(generated_text)
    print("\nã€æ ¼å¼æ£€æŸ¥ã€‘")
    print(f"  âœ“ åŒ…å« <context>: {'âœ…' if format_check['has_context'] else 'âŒ'}")
    print(f"  âœ“ åŒ…å« <think>:   {'âœ…' if format_check['has_think'] else 'âŒ'}")
    print(f"  âœ“ åŒ…å« <answer>:  {'âœ…' if format_check['has_answer'] else 'âŒ'}")
    print(f"  âœ“ æ ¼å¼å®Œæ•´:       {'âœ… æ­£ç¡®' if format_check['all_correct'] else 'âŒ é”™è¯¯'}")
    
    # 2. æå–å†…å®¹
    extracted = extract_tags(generated_text)
    
    print("\nã€ç”Ÿæˆå†…å®¹ã€‘")
    if extracted['context']:
        print(f"\n<context>")
        print(f"{extracted['context'][:300]}...")
        print(f"</context>")
    
    if extracted['think']:
        print(f"\n<think>")
        print(f"{extracted['think'][:300]}...")
        print(f"</think>")
    
    if extracted['answer']:
        print(f"\n<answer>")
        print(f"{extracted['answer']}")
        print(f"</answer>")
    
    # 3. ä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”
    ground_truth_solution = sample.get('solution', '')
    ground_truth_answer = sample.get('answer', '')
    
    print("\nã€æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”ã€‘")
    print(f"  Ground Truth Answer: {ground_truth_answer}")
    if extracted['answer']:
        print(f"  Generated Answer:    {extracted['answer']}")
        
        # ç®€å•çš„ç­”æ¡ˆåŒ¹é…
        if extracted['answer'].strip() == ground_truth_answer.strip():
            print(f"  åŒ¹é…ç»“æœ: âœ… å®Œå…¨åŒ¹é…")
        elif ground_truth_answer.strip() in extracted['answer'].strip():
            print(f"  åŒ¹é…ç»“æœ: âš ï¸ éƒ¨åˆ†åŒ¹é…")
        else:
            print(f"  åŒ¹é…ç»“æœ: âŒ ä¸åŒ¹é…")
    else:
        print(f"  Generated Answer:    âŒ æœªèƒ½æå–")
    
    # 4. å®Œæ•´è¾“å‡º
    print("\nã€å®Œæ•´ç”Ÿæˆæ–‡æœ¬ã€‘")
    print("-" * 80)
    print(generated_text)
    print("-" * 80)
    
    # ==================== ä¿å­˜ç»“æœ ====================
    # åˆ›å»ºlogsç›®å½•
    log_dir = Path("../logs")
    log_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = log_dir / f"basemodel_test_result_{timestamp}.json"
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    test_result = {
        "timestamp": timestamp,
        "model_path": MODEL_PATH,
        "sample_info": {
            "problem_type": sample.get('problem_type', 'unknown'),
            "data_type": sample.get('data_type', 'unknown'),
            "video_path": sample.get('path', 'unknown'),
            "question": format_question(sample),
            "ground_truth_answer": ground_truth_answer
        },
        "generated_output": {
            "full_text": generated_text,
            "context": extracted.get('context', ''),
            "think": extracted.get('think', ''),
            "answer": extracted.get('answer', '')
        },
        "evaluation": {
            "format_check": format_check,
            "answer_match": extracted['answer'].strip() == ground_truth_answer.strip() if extracted['answer'] else False
        }
    }
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(test_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ==================== æ€»ç»“ ====================
    print("\n" + "=" * 80)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("=" * 80)
    
    print("\nğŸ’¡ æç¤º:")
    print("  - å¦‚æœæ ¼å¼æ­£ç¡®ï¼Œè¯´æ˜æ¨¡å‹å­¦ä¼šäº†è¾“å‡ºç»“æ„")
    print("  - å¦‚æœç­”æ¡ˆåŒ¹é…ï¼Œè¯´æ˜æ¨¡å‹ç†è§£äº†ä»»åŠ¡")
    print("  - å¯ä»¥å¤šæ¬¡è¿è¡Œè„šæœ¬æµ‹è¯•ä¸åŒæ ·æœ¬")
    print()


if __name__ == "__main__":
    main()
